{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant for the students of Basics of Applied Mathematics, in the master of Mathematics in Data and Technology, Freiburg Univsersity, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise for Homework 10: Training a Classifier with Stochastic Gradient Descent in PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your task\n",
    "\n",
    "This notebook contains some code snippets with missing lines that you will have to fill in! Please also try to read and understand the parts that you do not have to fill in to understand better how the code works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will train a simple image classifier on the CIFAR10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html) using Stochastic Gradient Descent (SGD) in PyTorch.\n",
    "\n",
    "This exercise is inspired from the tutorial in https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to train a neural network to classify images from the CIFAR10 dataset into 10 different classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).\n",
    "\n",
    "The optimization problem takes the form:\n",
    "$$\n",
    "\\min_{x} \\frac{1}{N} \\sum_{i=1}^{N} \\text{criterion}(\\phi(\\text{image}_i; x), \\text{label}_i)\n",
    "$$\n",
    "where the function \\text{criterion}(\\text{prediction}, \\text{label}) is the cross-entropy loss function, that compares the prediction vector (of size 10) with the true label (an integer between 0 and 9).\n",
    "\n",
    "The model $\\phi(\\text{image}; x)$ is a Neural Network with parameters $x$ that takes as input an image (a tensor of size 3x32x32) and outputs a prediction vector (a tensor of size 10).\n",
    "This model is given already, and is defined in pytorch.\n",
    "The gradient of the objective is already given in a function called `compute_gradient`.\n",
    "\n",
    "Your task is to implement the SGD algorithm in the function `SGD` that uses the training dataset to optimize the parameters of the model.\n",
    "\n",
    "Ultimately, the performance is evaluated on a test dataset that is not used during training.\n",
    "\n",
    "You will also have to tune hyperparameters such that the learning is successful and fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khZmaFQPy1ps"
   },
   "outputs": [],
   "source": [
    "import numpy as np          # Package for numerical computing\n",
    "import matplotlib.pyplot as plt # Package for plotting\n",
    "from time import time # function to check runtime\n",
    "\n",
    "# Jupyter magic command to make the plots a bit nicer\n",
    "%matplotlib inline\n",
    "                     # use \"%matplotlib inline\" if one uses VS code UI\n",
    "                     # use \"%matplotlib notebook\" for Jupyter UI\n",
    "\n",
    "# You need to have pytorch and torchvision on your machine.\n",
    "# If you are using conda and struggle with the installation, consider creating a new conda environment dedicated to pytorch.\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write helper functions for printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display \n",
    "def nice_print(s): # this is for a better rendering of logs\n",
    "    display(Markdown(s))\n",
    "\n",
    "# Write a function to visualize some images along with their ground truth and predicted labels\n",
    "def show_images(images, labels, names, predicted_labels=None, image_treatment='gaussian', n_rows=1):\n",
    "    assert len(images) == len(labels), \"Number of images and labels must be the same\"\n",
    "    num = len(labels)\n",
    "    n_col = (num+n_rows-1) // n_rows\n",
    "    fig, axs = plt.subplots(n_rows, n_col, figsize=(n_col*1.5, n_rows*1.5))\n",
    "\n",
    "    indices = labels.argsort()  # indices to sort the images by their labels\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if i < num:\n",
    "            img = images[indices[i]]\n",
    "            label = labels[indices[i]]\n",
    "            ax.axis('off')\n",
    "            ax.imshow(img, interpolation=image_treatment)\n",
    "            ax.set_title(f\"{names[label]} \\n\", fontsize=10) \n",
    "            if predicted_labels is not None:\n",
    "                predicted_label = predicted_labels[indices[i]]\n",
    "                if predicted_label == label:\n",
    "                    ax.text(5, -2.1, f\"pred: {names[predicted_label]}\", color=\"green\", fontsize=8)    \n",
    "                else:\n",
    "                    ax.text(5, -2.1, f\"pred: {names[predicted_label]}\", color=\"red\", fontsize=8)\n",
    "        else:\n",
    "            ax.set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "colors = [\"black\", \"brown\", \"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\", \"C9\"]\n",
    "# Function to plot the losses and misclassification rates\n",
    "def plot_losses(list_infos, examples):     \n",
    "    fig, axs = plt.subplots(2, figsize=(10, 6))\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[1].set_ylabel('Misclassification rate')\n",
    "    axs[1].set_ylim(0., 1.)\n",
    "    idx_color = 0\n",
    "    for (name, val) in examples:\n",
    "        axs[1].axhline(val, color=colors[idx_color], linestyle='--', label=name)\n",
    "        idx_color += 1\n",
    "    for (label, epochs, losses, misclassification_rates) in list_infos:\n",
    "        axs[0].plot(epochs, losses, 'o-', label=label, color=colors[idx_color])\n",
    "        axs[1].plot(epochs, misclassification_rates, 'o-', label=label, color=colors[idx_color])\n",
    "        idx_color += 1\n",
    "    for ax in axs:\n",
    "        ax.grid()\n",
    "        ax.set_xlabel('Number of epochs')\n",
    "    axs[0].legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', download=True, train=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', download=True, train=False)\n",
    "names = list(train_dataset.classes)\n",
    "\n",
    "imgs_train = train_dataset.data # shape (50000, 32, 32, 3)\n",
    "labels_train = torch.tensor(train_dataset.targets) # shape (50000)\n",
    "print(f'The train dataset has {imgs_train.shape[0]} images of shape {imgs_train.shape[1:]}, classified into {len(names)} classes.')\n",
    "\n",
    "imgs_test = test_dataset.data # shape (10000, 32, 32, 3)\n",
    "labels_test = torch.tensor(test_dataset.targets) # shape (10000)\n",
    "print(f'The test dataset has {imgs_test.shape[0]} images of shape {imgs_test.shape[1:]}, classified into {len(names)} classes.')\n",
    "\n",
    "del train_dataset, test_dataset # free some memory\n",
    "\n",
    "# Function to convert images to torch tensors and scale them to [-1, 1]\n",
    "def img_to_tensor(imgs):\n",
    "    # imgs: (N, 32, 32, 3) Reorder to (N, 3, 32, 32), scale to [-1, 1], convert to torch tensor\n",
    "    imgs_ = np.transpose(imgs, (0, 3, 1, 2)).copy()          # (N, 3, 32, 32)\n",
    "    imgs_ = 2. * imgs_ / 255.0  - 1.0        # scale to [-1, 1]\n",
    "    return torch.from_numpy(imgs_).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a couple of images from the training dataset\n",
    "rng = np.random.default_rng(42)\n",
    "indices = rng.choice(imgs_train.shape[0], size=15, replace=False)\n",
    "fig = show_images(imgs_train[indices], labels_train[indices], names, n_rows=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssfFCaq0y1pv"
   },
   "source": [
    "### Define the neural network functions used for predicting the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "# Create a class for a pytorch neural network with two hidden layers\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, model_complexity=100, lambda_penalty=1e-4):\n",
    "        \"\"\"\n",
    "            Define a neural network with two hidden layers of 'model_complexity' neurons each.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_input = 3 * 32 * 32  # input dimension (3 color channels)\n",
    "        m = model_complexity # number of neurons in the hidden layers\n",
    "        n_output = 10\n",
    "        torch.manual_seed(0) # for reproducibility\n",
    "\n",
    "        self.nonlin_F = torch.nn.Softplus() # this function is like a smooth ReLU: f(x) = log(1 + exp(x))\n",
    "        self.lin1 = torch.nn.Linear(self.dim_input, m)  # 3*32*32 --> m  (with m = model_complexity)\n",
    "        self.lin2 = torch.nn.Linear(m, m)\n",
    "        self.lin3 = torch.nn.Linear(m, n_output)  # m --> n_output\n",
    "\n",
    "        self.x0 = parameters_to_vector(self.parameters()).detach()  # initial parameter vector\n",
    "        # Also store the crtiterion here\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.lambda_penalty = lambda_penalty # regularization parameter for weight decay\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input tensor has shape [N, 3, 32, 32]\n",
    "        x = input.reshape(-1, self.dim_input)  # flatten the input --> [N, 3*32*32]\n",
    "        layer1 = self.nonlin_F(self.lin1(x))  # --> [N, m]\n",
    "        layer2 = self.nonlin_F(self.lin2(layer1))  # --> [N, m]\n",
    "        output = self.lin3(layer2)  # --> [N, n_output]\n",
    "        return output  # raw scores for each class\n",
    "    \n",
    "    def compute_predictions(self, x, input):\n",
    "        vector_to_parameters(x, self.parameters())\n",
    "        with torch.no_grad():\n",
    "            return self(input)\n",
    "\n",
    "    def compute_gradient(self, x, input, output):\n",
    "        vector_to_parameters(x, self.parameters())\n",
    "        self.zero_grad() # reinitialize gradients to zero, else they will accumulate\n",
    "        prediction = self(input)\n",
    "        loss = self.criterion(prediction, output)\n",
    "        loss.backward()\n",
    "        grad = parameters_to_vector([p.grad for p in self.parameters() if p.grad is not None]).detach()\n",
    "        grad.add_(x, alpha=self.lambda_penalty)  # add gradient of the penalty term\n",
    "        return grad\n",
    "        \n",
    "    def loss_and_accuracy(self, x, inputs, outputs):\n",
    "        predictions = self.compute_predictions(x, inputs)\n",
    "        loss = self.criterion(predictions, outputs).item()\n",
    "        accuracy = (outputs == torch.argmax(predictions, dim=1)).sum().item() / outputs.size(0)\n",
    "        return loss, accuracy\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Stochastic Gradient Descent (SGD) with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   \n",
    "**Task:**\n",
    "\n",
    "Complete the following code to randomly split the indices $\\{0, \\ldots, N-1\\}$ into mini-batches of size `batch_size`. In the function, `rng` is a numpy random generator that you can use to generate random numbers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr/>\n",
    "<div class=\"alert alert-success\">\n",
    "   \n",
    "**Tips:**\n",
    "One way to do this is to first shuffle and then split.\n",
    "Note that the last batch may be smaller than `batch_size` if `N` is not divisible by `batch_size`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(N, batch_size, rng):\n",
    "    \"\"\"\n",
    "        This function returns a list of arrays, each containing the indices of a mini-batch. All mini-batch except for the last one have size `batch_size`.\n",
    "    \"\"\"\n",
    "    # ----- YOUR CODE\n",
    "    ...\n",
    "    # -----END YOUR CODE\n",
    "    return list_indices_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   \n",
    "**Task:**\n",
    "\n",
    "Complete the following code to implement min-batch Stochastic Gradient Descent (SGD) with momentum:\n",
    "$$\n",
    "    p_{k+1} = \\gamma p_{k} + (1-\\gamma) g_k(x_k) \\\\\n",
    "    x_{k+1} = x_k + \\alpha_k p_k\n",
    "$$\n",
    "with a decreasing step-size $\\alpha_k = \\frac{\\alpha_0}{\\sqrt{k_{ep.}}}$, where $k_{ep.}$ is the epoch number.\n",
    "The gradient estimate $g_k(x_k)$ is computed on a mini-batch of size $n_{batch}$:\n",
    "$$\n",
    "g_k(x_k) = \\frac{1}{| B_k |} \\sum_{i \\in B_k} \\nabla f_i(x_k)\n",
    "$$\n",
    "where $B_k \\subset \\{1, \\dots, N\\}$ is a random subset of indices of size $n_{batch}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr/>\n",
    "<div class=\"alert alert-success\">\n",
    "   \n",
    "**Tips:**\n",
    "You can use the syntax `grad = neural_net.compute_gradient(x, input_batch, output_batch)` to compute the gradient of the loss function for the input tensors `input_batch` and `output_batch`, at the parameter value `x`.\n",
    "\n",
    "You can also use the randomly generated initial parameters `x0` as a starting point for the optimization.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr/>\n",
    "<div class=\"alert alert-success\">\n",
    "   \n",
    "**Tips:**\n",
    "For a better performance, you might have a look at the methods , `tensor.mul_()`, `tensor.add_()` and `tensor.lerp_()` that perform in-place operations on tensors instead of creating new ones.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to perform SGD training\n",
    "def SGD(images, outputs, num_epochs=2, batch_size=1, alpha0=0.1, gamma=0.):\n",
    "    \"\"\"\n",
    "            Perform SGD training with mini-batches of size batch_size for num_epochs epochs, momentum parameter gamma, and decreasing step-size alpha = alpha0 / sqrt(k_epoch).\n",
    "            The function returns the list of of couples (k_e, x) where $k$ is the number of epoch and $x$ is the solution vector at that epoch.\n",
    "    \"\"\"\n",
    "    print(\"\\n \\n  ----------\")\n",
    "    name_optimizer = r\"SGD ; $n_{batch} = %s$ ; $\\alpha=\\frac{%s}{\\sqrt{k_{ep.}}}$, with momentum: $\\gamma = %s$\" % (batch_size, alpha0, gamma)\n",
    "    nice_print(name_optimizer)\n",
    "    t0 = time()\n",
    "    neural_net = NeuralNet()\n",
    "    inputs = img_to_tensor(images)\n",
    "    rng = np.random.default_rng(42) # random generator for the mini-batches\n",
    "    # ----- YOUR CODE (initialize some variables if needed)\n",
    "    ...\n",
    "    # --------- END YOUR CODE\n",
    "    print(f\"       Number of epoch = 0/{num_epochs}\", end=\" \")\n",
    "    for n_epoch in range(num_epochs):\n",
    "        iterates.append((n_epoch, x.clone()))  # store the iterate\n",
    "        list_indices_batch = random_split(N, batch_size, rng)\n",
    "        for indices in list_indices_batch:\n",
    "            # ----- YOUR CODE (perform one SGD update)\n",
    "            ...\n",
    "            # --------- END YOUR CODE\n",
    "        print(f\"{n_epoch+1}/{num_epochs}, \", end=\" \")\n",
    "    iterates.append((num_epochs, x.clone())) # store final iterate\n",
    "    print(f\"\\n   ... training finished! Runtime = {time() - t0 :.2f} sec.\")\n",
    "\n",
    "    return name_optimizer, iterates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   \n",
    "**Task:**\n",
    "\n",
    "Complete the following code to run the SGD algorithm with the hyperparameters of your choice.\n",
    "The code after plots the training loss and the test accuracy over the iterations, which should help you to choose good hyperparameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXIqSn5Yy1pw"
   },
   "outputs": [],
   "source": [
    "all_iterations = [] # should store couples (name, iterations), i.e. outputs of the SGD functions\n",
    "\n",
    "# ----- YOUR CODE (perform SGD for some hyperparameters)\n",
    "name, iterations = SGD(...)\n",
    "all_iterations.append((name, iterations))\n",
    "\n",
    "name, iterations = SGD(...)\n",
    "all_iterations.append((name, iterations))\n",
    "\n",
    "...\n",
    "# ----- END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the solutions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute some statistics about the performance on the test dataset for the solutions obtained\n",
    "def test_solutions(neural_net, x_solutions, inputs, outputs):\n",
    "    test_losses, accuracies = [], []\n",
    "    for x in x_solutions:\n",
    "        test_loss, accuracy = neural_net.loss_and_accuracy(x, inputs, outputs)\n",
    "        test_losses.append(test_loss)\n",
    "        accuracies.append(accuracy)\n",
    "    return test_losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_infos = []\n",
    "solutions = [] \n",
    "neural_net = NeuralNet()\n",
    "inputs_test = img_to_tensor(imgs_test)\n",
    "for name_opti, iterations in all_iterations:\n",
    "        epochs, iterates = zip(*iterations)\n",
    "        solution = iterates[-1].clone()\n",
    "        test_losses, accuracies = test_solutions(neural_net, iterates, inputs_test, labels_test)\n",
    "        solutions.append( (name_opti, solution) )\n",
    "        all_infos.append( (name_opti, epochs, test_losses, accuracies) )\n",
    "        nice_print(name_opti)\n",
    "        print(f\"    loss on test data: {test_losses[-1]:.2e}  accuracy on test data {accuracies[-1]*100:.2f}%\")\n",
    "del all_iterations # free some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [(\"random assignment\", 1/len(names)), (r\"50% accuracy\", 0.5)]\n",
    "fig = plot_losses(all_infos, examples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us have a look at the predictions on some of the images from the dataset\n",
    "nam_opti, x_sol = solutions[0]  # use the solution found by the best the first optimizer\n",
    "inputs_test = img_to_tensor(imgs_test)\n",
    "neural_net = NeuralNet()\n",
    "prediction = neural_net.compute_predictions(x_sol, inputs_test)\n",
    "predicted_labels = torch.argmax(prediction, dim=1)\n",
    "correctness = predicted_labels == labels_test\n",
    "\n",
    "# Now compute the counts for each class separately\n",
    "for i, name in enumerate(names):\n",
    "    indices_class = (labels_test == i)\n",
    "    num_in_class = indices_class.sum().item()\n",
    "    num_correct_in_class = (correctness & indices_class).sum().item()\n",
    "    print(f\"   Class '{name:10s}': {num_correct_in_class:5d} correct / {num_in_class} --> accuracy: {num_correct_in_class/num_in_class*100:.2f}%\")\n",
    "\n",
    "print(f\"Total accuracy on test set: {correctness.sum().item()/correctness.size(0)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a couple of images from the training dataset\n",
    "rng = np.random.default_rng(50)\n",
    "indices = rng.choice(imgs_test.shape[0], size=15, replace=False)\n",
    "fig = show_images(imgs_test[indices], labels_test[indices], names, predicted_labels=predicted_labels[indices], n_rows=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
